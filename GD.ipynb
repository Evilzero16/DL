{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Evilzero16/DL/blob/main/GD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp3_T27EpmTT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from numpy.random import permutation\n",
        "\n",
        "class Line():\n",
        "    \"\"\"\n",
        "        Linear Model with two weights w0 (intercept) and w1 (slope)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.weights = [np.random.uniform(0,1,1) for _ in range(2)]\n",
        "        self.derivative_funcs = [self.dx_w0, self.dx_w1]\n",
        "        \n",
        "    def evaluate(self,x):\n",
        "        \"\"\"\n",
        "            evaluate: will evaluate the line yhate given x\n",
        "            x: a point in the plane\n",
        "\n",
        "            return the result of the function evalutation\n",
        "        \"\"\"\n",
        "        return self.weights[0] + self.weights[1]*x\n",
        "\n",
        "    def derivate(self, x, y):\n",
        "        \"\"\"\n",
        "            derivate: will calculate all partial derivatives and return them\n",
        "            input:\n",
        "            x: a point in the plane\n",
        "            y: the response of the point x\n",
        "            \n",
        "            output:\n",
        "            partial_derivatives: an array of partial derivatives\n",
        "        \"\"\"\n",
        "        partial_derivatives = []\n",
        "        \n",
        "        yhat = self.evaluate(x)\n",
        "        partial_derivatives.append(self.dx_w0(x, y, yhat))\n",
        "        partial_derivatives.append(self.dx_w1(x, y, yhat))\n",
        "        \n",
        "        return partial_derivatives\n",
        "    \n",
        "    def dx_w0(self, x, y, yhat):\n",
        "        \"\"\"\n",
        "            dx_w0: partial derivative of the weight w0\n",
        "            x: a point in the plane\n",
        "            y: the response of the point x\n",
        "            yhat: the current approximation of y given x and the weights\n",
        "\n",
        "            return the gradient at that point for this x and y for w0\n",
        "        \"\"\"\n",
        "        return 2*(yhat - y)\n",
        "    \n",
        "    def dx_w1(self, x, y, yhat):\n",
        "        \"\"\"\n",
        "            dx_w1: partial derivative of the weight w1 for a linear function\n",
        "            x: a point in the plane\n",
        "            y: the response of the point x\n",
        "            yhat: the current approximation of y given x and the weights\n",
        "\n",
        "            return the gradient at that point for this x and y for w1\n",
        "        \"\"\"  \n",
        "        return 2*x*(yhat - y)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"y = {self.weights[0]} + {self.weights[1]}*x\"\n",
        "        \n",
        "    \n",
        "#################### Helper functions ######################\n",
        "def stochastic_sample(xs, ys):\n",
        "    \"\"\"\n",
        "        stochastic_sample: sample with replacement one x and one y\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        \n",
        "        return the randomly selected x and y point\n",
        "    \"\"\"\n",
        "    perm = permutation(len(xs))\n",
        "    x = xs[perm[0]]\n",
        "    y = ys[perm[0]]\n",
        "\n",
        "    return x, y\n",
        "    \n",
        "    \n",
        "def gradient(dx, evaluate, xs, ys):\n",
        "    \"\"\"\n",
        "        gradient: estimate mean gradient over all point for w1\n",
        "        evaluate: the evaulation function from the model\n",
        "        dx: partial derivative function used to evaluate the gradient\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        \n",
        "        return the mean gradient all x and y for w1\n",
        "    \"\"\"         \n",
        "    N = len(ys)\n",
        "    \n",
        "    total = 0\n",
        "    for x,y in zip(xs,ys):\n",
        "        yhat = evaluate(x)\n",
        "        total = total + dx(x, y, yhat)\n",
        "    \n",
        "    gradient = total/N\n",
        "    return gradient\n",
        "\n",
        "################## Optimization Functions #####################\n",
        "\n",
        "def gd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
        "    \"\"\"\n",
        "        gd: will estimate the parameters w1 and w2 (here it uses least square cost function)\n",
        "        model: the model we are trying to optimize using gradient descent\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        learning_rate: the learning rate for the step that weights update will take\n",
        "        max_num_iteration: the number of iteration before we stop updating\n",
        "    \"\"\"    \n",
        "\n",
        "    for i in range(max_num_iteration):\n",
        "        # Updating the model parameters\n",
        "        model.weights = [weight - learning_rate*gradient(derivative_func, model.evaluate, xs, ys) for weight, derivative_func in zip(model.weights, model.derivative_funcs)]\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "            print(model)\n",
        "            \n",
        "def sgd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
        "    \"\"\"\n",
        "        sgd: will estimate the parameters w0 and w1 \n",
        "        (here it uses least square cost function)\n",
        "        model: the model we are trying to optimize using sgd\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        learning_rate: the learning rate for the step that weights update will take\n",
        "        max_num_iteration: the number of iteration before we stop updating\n",
        "    \"\"\"       \n",
        "    \n",
        "    for i in range(max_num_iteration):\n",
        "        \n",
        "        # Select a random x and y\n",
        "        x, y = stochastic_sample(xs, ys)\n",
        "        \n",
        "        # Updating the model parameters\n",
        "        model.weights = [weight - learning_rate*derivative for weight, derivative in zip(model.weights, model.derivate(x,y))]        \n",
        "    \n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "            print(model)\n",
        "            \n",
        "def sgd_momentum(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 1000):\n",
        "    \"\"\"\n",
        "        sgd_momentum: will estimate the parameters w0 and w1 \n",
        "        (here it uses least square cost function)\n",
        "        model: the model we are trying to optimize using sgd\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        learning_rate: the learning rate for the step that weights update will take\n",
        "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
        "        max_num_iteration: the number of iteration before we stop updating\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create the gradient that we keep track as an array of 0 of the same size as the number of weights\n",
        "    gradients = [0 for _ in range(len(model.weights))]\n",
        "    \n",
        "    for i in range(max_num_iteration):\n",
        "        \n",
        "        # Select a random x and y\n",
        "        x, y = stochastic_sample(xs, ys)\n",
        "\n",
        "        # Calculate the new gradients\n",
        "        gradients = [decay_factor*g + learning_rate*derivative for g, derivative in zip(gradients, model.derivate(x,y))]\n",
        "        \n",
        "        # Updating the model parameters\n",
        "        model.weights = [weight - g for weight, g in zip(model.weights, gradients)]\n",
        "    \n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "            print(model)\n",
        "            \n",
        "            \n",
        "def adagrad(model, xs, ys, learning_rate = 0.1, max_num_iteration = 1000, eps=0.0000001):\n",
        "    \"\"\"\n",
        "        adagrad: will estimate the parameters w0 and w1 \n",
        "        (here it uses least square cost function)\n",
        "        model: the model we are trying to optimize using sgd\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        learning_rate: the learning rate for the step that weights update will take\n",
        "        max_num_iteration: the number of iteration before we stop updating\n",
        "        eps: is a numerical safety to avoid division by 0\n",
        "    \"\"\"         \n",
        "    \n",
        "    # Here only the diagonal matter\n",
        "    num_param = len(model.weights)\n",
        "    G = [[0 for _ in range(num_param)] for _ in range(num_param)]\n",
        "    \n",
        "    for i in range(max_num_iteration):\n",
        "        \n",
        "        # Select a random x and y\n",
        "        x, y = stochastic_sample(xs, ys)\n",
        "        \n",
        "        # Update G and the model weights iteratively (Note: speed up could be gained from vectorized implementation)\n",
        "        for idx, gradient in enumerate(model.derivate(x, y)):\n",
        "            G[idx][idx] = G[idx][idx] + gradient**2\n",
        "            model.weights[idx] = model.weights[idx] - (learning_rate / np.sqrt(G[idx][idx] + eps)) * gradient\n",
        "    \n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "            print(model)\n",
        "            \n",
        "def rmsprop(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 10000, eps=0.0000001):\n",
        "    \"\"\"\n",
        "        rmsprop: will estimate the parameters w0 and w1 \n",
        "        (here it uses least square cost function)\n",
        "        model: the model we are trying to optimize using sgd\n",
        "        xs: all point on the plane\n",
        "        ys: all response on the plane\n",
        "        learning_rate: the learning rate for the step that weights update will take\n",
        "        decay_factor: the parameter used in the running averaging\n",
        "        max_num_iteration: the number of iteration before we stop updating\n",
        "        eps: is a numerical safety to avoid division by 0\n",
        "    \"\"\"         \n",
        "    \n",
        "    # Running average\n",
        "    E = [0 for _ in range(len(model.weights))]\n",
        "    \n",
        "    for i in range(max_num_iteration):\n",
        "        \n",
        "        # Select a random x and y\n",
        "        x, y = stochastic_sample(xs, ys)\n",
        "        \n",
        "        # Update E and the model weights iteratively (Note: speed up could be gained from vectorized implementation)\n",
        "        for idx, gradient in enumerate(model.derivate(x, y)):    \n",
        "            E[idx] = decay_factor*E[idx] + (1 - decay_factor)*(gradient**2)\n",
        "            model.weights[idx] = model.weights[idx] - (learning_rate/np.sqrt(E[idx] + eps))*gradient\n",
        "\n",
        "    \n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "            print(model)\n",
        "\n",
        "def adam(model, xs, ys, learning_rate = 0.1, b1 = 0.9, b2 = 0.999, epsilon = 0.00000001, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "        Adam: This is the adam optimizer that build upon adadelta and RMSProp\n",
        "        model: The model we want to optimize the parameter on\n",
        "        xs: the feature of my dataset\n",
        "        ys: the continous value (target)\n",
        "        learning_rate: the amount of learning we want to happen at each time step (default is 0.1 and will be updated by the optimizer)\n",
        "        b1: this is the first decaying average with proposed default value of 0.9 (deep learning purposes)\n",
        "        b2: this is the second decaying average with proposed default value of 0.999 (deep learning purposes)\n",
        "        epsilon: a variable for numerical stability during the division\n",
        "        max_iteration: the number of sgd round we want to do before stopping the optimization\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # Variable Initialization\n",
        "    num_param = len(model.weights)\n",
        "    m = [0 for _ in range(num_param)] # two m for each parameter\n",
        "    v = [0 for _ in range(num_param)] # two v for each parameter\n",
        "    g = [0 for _ in range(num_param)] # two gradient\n",
        "    \n",
        "    for t in range(1,max_iteration):\n",
        "        \n",
        "        # Calculate the gradients \n",
        "        x, y = stochastic_sample(xs, ys)\n",
        "        \n",
        "        # Get the partial derivatives\n",
        "        g = model.derivate(x, y)\n",
        "\n",
        "        # Update the m and v parameter\n",
        "        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]\n",
        "        v = [b2*v_i + (1 - b2)*(g_i**2) for v_i, g_i in zip(v, g)]\n",
        "\n",
        "        # Bias correction for m and v\n",
        "        m_cor = [m_i / (1 - (b1**t)) for m_i in m]\n",
        "        v_cor = [v_i / (1 - (b2**t)) for v_i in v]\n",
        "\n",
        "        # Update the parameter\n",
        "        model.weights = [weight - (learning_rate / (np.sqrt(v_cor_i) + epsilon))*m_cor_i for weight, v_cor_i, m_cor_i in zip(model.weights, v_cor, m_cor)]\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(f\"Iteration {t}\")\n",
        "            print(model)\n",
        "    \n",
        "def nesterov(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 1000):\n",
        "    \"\"\"\n",
        "        Nesterov: This is the nesterov accelerated gradient optimizer that build upon momentum\n",
        "        model: the model we want to optimize the parameter on (this is a line right now)\n",
        "        xs: the feature of my dataset\n",
        "        ys: the continous value (target)\n",
        "        learning_rate: the learning rate for the step that weights update will take\n",
        "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
        "        max_num_iteration: the number of iteration before we stop updating\n",
        "    \"\"\"\n",
        "    \n",
        "    # These are needed to keep track of the previous gradient\n",
        "    g = [0 for _ in range(len(model.weights))] \n",
        "    \n",
        "    for i in range(max_num_iteration):\n",
        "        \n",
        "        # Select a random x and y\n",
        "        x, y = stochastic_sample(xs, ys)\n",
        "\n",
        "        # Calculate the gradient for w0 by predicting where the ball will be (approximatively)\n",
        "        for idx, gradient in enumerate(model.derivate(x,y)):\n",
        "            \n",
        "            # Here we need to do a bit of gymnastic because of how the code is setup\n",
        "            # We need to save the parameters state, modify it, do the simulation and then reset the parameter state\n",
        "            # The update happen in the next section\n",
        "            prev_weight = model.weights[idx]\n",
        "            model.weights[idx] = decay_factor*gradient\n",
        "            g[idx] = decay_factor*g[idx] + learning_rate*gradient\n",
        "            model.weights[idx] = prev_weight\n",
        "            \n",
        "            # Update the model parameter\n",
        "            model.weights[idx] = model.weights[idx] - g[idx]\n",
        "            \n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "            print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we have a simple line with intercept = 0 and slope = 1\n",
        "xs = [1,2,3,4,5,6,7]\n",
        "ys = [1,2,3,4,5,6,7]\n",
        "\n",
        "print(\"Target: intercept = 0 and slope = 1\")\n",
        "\n",
        "\n",
        "# # Gradient Descent\n",
        "# model = Line()\n",
        "# print(\"Gradient Descent: \")\n",
        "# gd(model, xs, ys)\n",
        "# print(model)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "model = Line()\n",
        "print(\"Stochastic Gradient Descent: \")\n",
        "sgd(model, xs, ys)\n",
        "print(model)\n",
        "\n",
        "# # Stochastic Gradient Descent with Momentum\n",
        "# model = Line()\n",
        "# print(\"SGD + Momentum: \")\n",
        "# sgd_momentum(model, xs, ys)\n",
        "# print(model)\n",
        "\n",
        "\n",
        "# # Adagrad\n",
        "# model = Line()\n",
        "# print(\"Adagrad\")\n",
        "# adagrad(model, xs, ys)\n",
        "# print(model)\n",
        "\n",
        "# # RMSprop\n",
        "# model = Line()\n",
        "# print(\"RMSprop\")\n",
        "# rmsprop(model, xs, ys)\n",
        "# print(model)\n",
        "\n",
        "# # Adam\n",
        "# model = Line()\n",
        "# print(\"Adam\")\n",
        "# adam(model, xs, ys)\n",
        "# print(model)\n",
        "\n",
        "# # Nesterov Accelerated Gradient\n",
        "# model = Line()\n",
        "# print(\"Nesterov Accelerated Gradient\")\n",
        "# nesterov(model, xs, ys)\n",
        "# print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2BVAznMq5DB",
        "outputId": "ca1b1cc8-4eed-47e8-889c-d697b8d8de78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: intercept = 0 and slope = 1\n",
            "Stochastic Gradient Descent: \n",
            "Iteration 0\n",
            "y = [0.85067745] + [0.45612767]*x\n",
            "Iteration 100\n",
            "y = [0.61852357] + [0.86383968]*x\n",
            "Iteration 200\n",
            "y = [0.41457415] + [0.90720916]*x\n",
            "Iteration 300\n",
            "y = [0.3058466] + [0.94680929]*x\n",
            "Iteration 400\n",
            "y = [0.21054131] + [0.94893116]*x\n",
            "Iteration 500\n",
            "y = [0.14755623] + [0.95979954]*x\n",
            "Iteration 600\n",
            "y = [0.10469542] + [0.97817653]*x\n",
            "Iteration 700\n",
            "y = [0.07060464] + [0.98570143]*x\n",
            "Iteration 800\n",
            "y = [0.04491438] + [0.99181598]*x\n",
            "Iteration 900\n",
            "y = [0.03322988] + [0.99324186]*x\n",
            "y = [0.02414387] + [0.99522709]*x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[np.random.uniform(0,1,1) for _ in range(2)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shGVXwccrOQh",
        "outputId": "ed919f81-5dea-481e-ed8d-1151a4e8edbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0.09642559]), array([0.12228897])]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perm = permutation(5)\n",
        "perm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRE7GWsprgLS",
        "outputId": "df4c5ce6-334d-44e4-acae-89aec16ac365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 4, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1, 2, 3, 4]\n",
        "y = [5, 6, 7, 8]\n",
        "\n",
        "for i in zip(x,y):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlNgkTf2sBF-",
        "outputId": "5f268c36-368b-4a45-f645-0c6267f702ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n",
            "(2, 6)\n",
            "(3, 7)\n",
            "(4, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stohastic GD"
      ],
      "metadata": {
        "id": "FXXxK7RBtzY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [1,2,3,4,5,6,7]\n",
        "ys = [1,2,3,4,5,6,7]"
      ],
      "metadata": {
        "id": "4h96XPWyyT5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fx = weights[0] + weights[1]*x\n",
        "\n",
        "# y = w0 + w1*x\n",
        "\n",
        "def fx(weights, x):\n",
        "  return weights[0] + weights[1]*x"
      ],
      "metadata": {
        "id": "ZpmxjQMoty3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_sample(xs, ys):\n",
        "  \n",
        "  id = np.random.randint(len(xs))\n",
        "  x = xs[id]\n",
        "  y = ys[id]\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "F31wGxcCyA1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = (y-y_hat)**2\n",
        "# dl/dw0 = 2(y-y_hat)\n",
        "# dl/dw1 = 2x(y-y_hat)\n",
        "\n",
        "def dx_w0(x, y, y_hat):\n",
        "  return 2*(y_hat-y)\n",
        "\n",
        "def dx_w1(x, y, y_hat):\n",
        "  return 2*x*(y_hat-y)"
      ],
      "metadata": {
        "id": "G9zTk9g8xtKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update at every data point\n",
        "def sgd(xs, ys, weights, derivative_funcs, max_epochs = 1000, lr = 0.001):\n",
        "  \n",
        "  for i in range(max_epochs):\n",
        "    \n",
        "    x, y = stochastic_sample(xs, ys)\n",
        "\n",
        "    y_hat = fx(weights, x)\n",
        "\n",
        "    weights[0] = weights[0] - lr * derivative_funcs[0](x, y, y_hat)\n",
        "    weights[1] = weights[1] - lr * derivative_funcs[1](x, y, y_hat)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(f\"Equation ---> y = {weights[0]} + {weights[1]}*x\")"
      ],
      "metadata": {
        "id": "2YWEw0H7zQ82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  #  total = 0\n",
        "  #   for x,y in zip(xs,ys):\n",
        "  #       yhat = evaluate(x)\n",
        "  #       total = total + dx(x, y, yhat)\n",
        "    \n",
        "  #   gradient = total/N"
      ],
      "metadata": {
        "id": "AyOKuzcB2mcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [np.random.rand(), np.random.rand()]\n",
        "derivative_funcs = [dx_w0, dx_w1]"
      ],
      "metadata": {
        "id": "eLFEoBPbt6UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd(xs, ys, weights, derivative_funcs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHeNxAhb1tCQ",
        "outputId": "33cb87aa-3f71-491f-fa37-05be6cd491ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equation ---> y = [0.11524274] + [0.61397645]*x\n",
            "Equation ---> y = [0.17901088] + [0.95703724]*x\n",
            "Equation ---> y = [0.1731139] + [0.96584744]*x\n",
            "Equation ---> y = [0.1662497] + [0.96440353]*x\n",
            "Equation ---> y = [0.15917359] + [0.96762779]*x\n",
            "Equation ---> y = [0.15220276] + [0.96751028]*x\n",
            "Equation ---> y = [0.14518989] + [0.97015811]*x\n",
            "Equation ---> y = [0.14017751] + [0.97052684]*x\n",
            "Equation ---> y = [0.13538963] + [0.97549695]*x\n",
            "Equation ---> y = [0.12994668] + [0.97420925]*x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAG SGD"
      ],
      "metadata": {
        "id": "eu4_rNlU4901"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [np.random.rand(), np.random.rand()]\n",
        "derivative_funcs = [dx_w0, dx_w1]"
      ],
      "metadata": {
        "id": "JSf5xvEO8tLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update rule uses uw and wb, that store history of all gradients, \n",
        "# multiplied by a decay constant + learning_rate * new gradient\n",
        "\n",
        "def nagsgd(xs, ys, weights, derivative_funcs, max_epochs = 100, lr = 0.00000001, decay = 0.99):\n",
        "  \n",
        "  uw0, uw1 = 0, 0\n",
        "\n",
        "  for i in range(max_epochs):\n",
        "    \n",
        "    x, y = stochastic_sample(xs, ys)\n",
        "\n",
        "    y_hat = fx([weights[0]-uw0, weights[1]-uw1], x)\n",
        "\n",
        "    uw0 += decay*uw0 + derivative_funcs[0](x, y, y_hat)\n",
        "    uw1 += decay*uw1 + derivative_funcs[1](x, y, y_hat)\n",
        "\n",
        "    weights[0] = weights[0] - lr*uw0\n",
        "    weights[1] = weights[1] - lr*uw1\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      print(f\"Equation ---> y = {weights[0]} + {weights[1]}*x\")"
      ],
      "metadata": {
        "id": "oPzHIDAx2FS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msgd(xs, ys, weights, derivative_funcs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ7GQAwv8ZCM",
        "outputId": "3139f23f-df07-4b80-dcef-5bd882091f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equation ---> y = 0.8856692219926035 + 0.19674179881252443*x\n",
            "Equation ---> y = 168587.0020745633 + 639823.7147549607*x\n",
            "Equation ---> y = 5.256896916450054e+17 + 3.4670931793860675e+18*x\n",
            "Equation ---> y = 6.351180658560999e+31 + 4.6249098522221065e+32*x\n",
            "Equation ---> y = 1.8243817606364102e+46 + 1.265645851062569e+47*x\n",
            "Equation ---> y = -6.309875370762533e+57 + 2.603935315057803e+58*x\n",
            "Equation ---> y = 1.6515712805958813e+72 + 6.480863548897396e+72*x\n",
            "Equation ---> y = 9.19209667345592e+87 + 2.4906030703807563e+88*x\n",
            "Equation ---> y = 3.691887757674088e+100 + 5.355338162516061e+100*x\n",
            "Equation ---> y = 1.7683408182771098e+113 + 1.0849834907565852e+114*x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htZe4KUW8fAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADAM"
      ],
      "metadata": {
        "id": "WACgRvpn-gpS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jn5yoVM8ERyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}